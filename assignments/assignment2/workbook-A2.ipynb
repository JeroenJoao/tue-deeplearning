{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vlamen/tue-deeplearning/blob/main/assignments/assignment2/workbook-A2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7-y662f1PHV"
   },
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UfUkWhex5pLP"
   },
   "outputs": [],
   "source": [
    "# Install PyTorch Geometric\n",
    "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
    "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
    "!pip install -q torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-3iwORqz9BQJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "import io\n",
    "import pickle\n",
    "import json\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download(url, filetype='tensor'):\n",
    "    if filetype not in ['tensor', 'pickle', 'json']:\n",
    "        raise ValueError('Incorrect filetype')\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    if filetype == 'tensor':\n",
    "        return torch.load(io.BytesIO(response.content))\n",
    "    elif filetype == 'pickle':\n",
    "        return pickle.load(io.BytesIO(response.content))\n",
    "    elif filetype == 'json':\n",
    "        return json.load(io.BytesIO(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtLMtRZa1Vjy"
   },
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3h2zBMkm1XpD"
   },
   "outputs": [],
   "source": [
    "## Download the dataset for image retrieval ##\n",
    "data_1 = download('https://surfdrive.surf.nl/files/index.php/s/EH2tN7JiZnwdIXg/download', filetype='tensor')\n",
    "data_metadata = download('https://github.com/pmernyei/wiki-cs-dataset/raw/master/dataset/metadata.json', filetype='json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1621940467107,
     "user": {
      "displayName": "Yoeri Poels",
      "photoUrl": "",
      "userId": "12918185417432069249"
     },
     "user_tz": -120
    },
    "id": "rwHKWhIF90JE",
    "outputId": "031a7857-60ba-4acc-9803-e0f98558cf33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num nodes: 10701\n",
      "Num edges: 251927\n",
      "\n",
      "Num node features: 300\n",
      "Num classes: 10\n",
      "\n",
      "Num training labels: 522\n"
     ]
    }
   ],
   "source": [
    "num_node = data_1.x.shape[0]\n",
    "num_edge = data_1.edge_index.shape[1]\n",
    "num_node_feature = data_1.x.shape[1]\n",
    "num_class = int(max(data_1.y)+1)\n",
    "num_label = sum(data_1.train_mask)\n",
    "\n",
    "print(f'Num nodes: {num_node}')\n",
    "print(f'Num edges: {num_edge}')\n",
    "print()\n",
    "print(f'Num node features: {num_node_feature}')\n",
    "print(f'Num classes: {num_class}')\n",
    "print()\n",
    "print(f'Num training labels: {num_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1621940467107,
     "user": {
      "displayName": "Yoeri Poels",
      "photoUrl": "",
      "userId": "12918185417432069249"
     },
     "user_tz": -120
    },
    "id": "ZkCGKmY0CPh2",
    "outputId": "49bf2307-7898-42c2-a95f-97a42a7772fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Computational linguistics\n",
      "1: Databases\n",
      "2: Operating systems\n",
      "3: Computer architecture\n",
      "4: Computer security\n",
      "5: Internet protocols\n",
      "6: Computer file systems\n",
      "7: Distributed computing architecture\n",
      "8: Web technology\n",
      "9: Programming language topics\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_class):\n",
    "    print('{}: {}'.format(i, data_metadata['labels'][str(i)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7pDn_vdhCNPw"
   },
   "outputs": [],
   "source": [
    "def print_node(idx, url=True, label=True):\n",
    "    node_info = data_metadata['nodes'][idx]\n",
    "    print(f'--Node {idx}--')\n",
    "    if url:\n",
    "        title = node_info['title']\n",
    "        wiki_url = 'https://en.wikipedia.org/wiki/' + title\n",
    "        print(wiki_url)\n",
    "    if label:\n",
    "        print('Label:', node_info['label'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1621940467108,
     "user": {
      "displayName": "Yoeri Poels",
      "photoUrl": "",
      "userId": "12918185417432069249"
     },
     "user_tz": -120
    },
    "id": "hvCmB6tFCr4R",
    "outputId": "d692d542-54ad-4706-945c-80a30ca78643"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Node 0--\n",
      "https://en.wikipedia.org/wiki/Twilio\n",
      "Label: Distributed computing architecture\n",
      "\n",
      "--Node 1--\n",
      "https://en.wikipedia.org/wiki/Program_compatibility_date_range\n",
      "Label: Operating systems\n",
      "\n",
      "--Node 2--\n",
      "https://en.wikipedia.org/wiki/SYSTAT_(DEC)\n",
      "Label: Operating systems\n",
      "\n",
      "--Node 3--\n",
      "https://en.wikipedia.org/wiki/List_of_column-oriented_DBMSes\n",
      "Label: Databases\n",
      "\n",
      "--Node 4--\n",
      "https://en.wikipedia.org/wiki/Stealth_wallpaper\n",
      "Label: Computer security\n",
      "\n",
      "--Node 5--\n",
      "https://en.wikipedia.org/wiki/Scalable_TCP\n",
      "Label: Internet protocols\n",
      "\n",
      "--Node 6--\n",
      "https://en.wikipedia.org/wiki/Carrier_IQ\n",
      "Label: Computer security\n",
      "\n",
      "--Node 7--\n",
      "https://en.wikipedia.org/wiki/ACF2\n",
      "Label: Operating systems\n",
      "\n",
      "--Node 8--\n",
      "https://en.wikipedia.org/wiki/Dorkbot_(malware)\n",
      "Label: Computer security\n",
      "\n",
      "--Node 9--\n",
      "https://en.wikipedia.org/wiki/Lout_(software)\n",
      "Label: Programming language topics\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print_node(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "IiWSjUmW1ZQ2"
   },
   "outputs": [],
   "source": [
    "## Build the article retrieval system ##\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "num_features = int(data_1.x.shape[1])\n",
    "num_classes = int(max(data_1.y)+1)\n",
    "\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(num_features, hidden_channels)  # the node's word vector (with size num_features) is  \n",
    "                                                              # transformed to a vector of size hidden_channels\n",
    "\n",
    "        self.conv2 = SAGEConv(hidden_channels, num_classes)  # we convert our nodes from hidden_channels to num_classes, the\n",
    "                                                             # last update step turns a node's state into a node-class prediction\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x = (1), all nodes contain information about itself (x_size, num_features)\n",
    "        x = self.conv1(x, edge_index) # x = (2), update all nodes for the first time, returning (x_size, hidden_channels)\n",
    "        x = x.relu()  # activation function\n",
    "        x = F.dropout(x, p=0.7, training=self.training)  # attempt to combat overfitting, as we only have few labels\n",
    "        x = self.conv2(x, edge_index)  # x = (3), update all nodes again, returning (x_size, num_classes)\n",
    "                                       # this final update transforms each node embedding to a class prediction\n",
    "                                       # we do not apply an activation, as the PyTorch CCE calculation\n",
    "                                       # takes care of treating this output as 'softmax'.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nxwb70w6BLzc"
   },
   "outputs": [],
   "source": [
    "## Evaluate the article retrieval system ##\n",
    "model_gnn = GraphSAGE(hidden_channels=16) # initialize our GNN with a hidden size of 16\n",
    "print(model_gnn)\n",
    "\n",
    "# same loss and optimizer as before\n",
    "loss_func = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model_gnn.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "def train_gnn():\n",
    "    model_gnn.train()  # set the model to training 'mode' (i.e., apply dropout)\n",
    "    optimizer.zero_grad()  # set gradients to 0\n",
    "    out = model_gnn(data_1.x, data_1.edge_index)  # propagate the data through the model\n",
    "    loss = loss_func(out[data_1.train_mask], data_1.y[data_1.train_mask])  # compute the loss based on our training mask\n",
    "    loss.backward()  # derive gradients\n",
    "    optimizer.step()  # update all parameters based on the gradients\n",
    "    return loss\n",
    "\n",
    "def test_gnn(mask):\n",
    "    model_gnn.eval()  # set the model to evaluation 'mode' (don't use dropout)\n",
    "    out = model_gnn(data_1.x, data_1.edge_index)  # propagate the data through the model\n",
    "    pred = out.argmax(dim=1)  # as prediction, we take the class with the highest probability\n",
    "    test_correct = pred[mask] == data_1.y[mask]  # create a tensor that evaluates whether predictions were correct\n",
    "    test_acc = int(test_correct.sum()) / int(mask.sum())  # get the accuracy\n",
    "    return test_acc\n",
    "\n",
    "\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "epochs = 50\n",
    "for epoch in range(1, epochs+1): \n",
    "    loss = train_gnn()  # do one training step over the entire dataset\n",
    "    train_acc = test_gnn(data_1.train_mask)  # compute the training accuracy\n",
    "    test_acc = test_gnn(data_1.test_mask)  # compute the test accuracy\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "    train_accs.append(train_acc)  # save accuracies so we can plot them\n",
    "    test_accs.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "To get an overview of the accuracy over the training period, we define a simple function\n",
    "to plot the saved accuracies.\n",
    "'''\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_train(train_accs, test_accs):\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    fnt=16\n",
    "    ax.plot(train_accs, color='blue', label='Train')\n",
    "    ax.plot(test_accs, color='red', linestyle='--', label='Test')\n",
    "    ax.legend(fontsize=fnt)\n",
    "    ax.tick_params(axis='both', labelsize=fnt)\n",
    "    ax.set_xlabel('Epoch', fontsize=fnt)\n",
    "    ax.set_ylabel('Accuracy', fontsize=fnt)\n",
    "\n",
    "plot_train(train_accs, test_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import cm\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "'''\n",
    "A function that plots a models embeddings, and colors them either according to \n",
    "the original labels or the predicted labels. Additionally, we can highlight \n",
    "individual datapoints in this embedding.\n",
    "'''\n",
    "def visualize(y=None, out=None, z=None, hl=None, text_type='idx', return_z=False, color_pred=False):\n",
    "    if out is None and z is None:\n",
    "        print('Must supply either model or TSNE output!')\n",
    "        return\n",
    "    if (y is None and out is None) or (y is None and color_pred is False):\n",
    "        print('Must supply either y or calculate y from out!')\n",
    "        return\n",
    "\n",
    "    if z is None: # create our embedding if it was not provided\n",
    "        z = TSNE(n_components=2).fit_transform(out.detach().cpu().numpy())\n",
    "    \n",
    "    if y is None and color_pred is True:\n",
    "        y = out.argmax(dim=1) # if no labels are provided, take the model's output\n",
    "    \n",
    "    # initialize our figure / plotting related settings\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches((10, 10))\n",
    "    \n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    colors = list(plt.cm.Paired.colors)[0:num_classes]\n",
    "    labels = [idx_to_label[i] for i in range(num_classes)]\n",
    "    \n",
    "    c = [colors[i] for i in y]\n",
    "    \n",
    "    s=50\n",
    "    scatter = ax.scatter(z[:, 0], z[:, 1], s=s, c=c)\n",
    "            \n",
    "    # create our legend\n",
    "    handles = [Patch(color=c, label=l) for c, l in zip(colors, labels)]\n",
    "    ax.legend(handles=handles)\n",
    "    plt.show()\n",
    "\n",
    "#label_to_idx = {num_class[i]: i for i in range(len(labels))}\n",
    "idx_to_label = {i: data_metadata['labels'][str(i)] for i in range(num_classes)}\n",
    "\n",
    "out = model_gnn(data_1.x, data_1.edge_index)  # get node embeddings from the GNN\n",
    "z = TSNE(n_components=2).fit_transform(out.detach().cpu().numpy())  # and create a TSNE visualization\n",
    "visualize(z=z, y=data_1.y, hl=[10], text_type='title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-VtEksc1fCt"
   },
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_-TRngWF1ggw"
   },
   "outputs": [],
   "source": [
    "## Build the anomaly detection model ##\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc_input = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_hidden = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_sigma = nn.Linear (hidden_dim, latent_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = torch.relu(self.fc_input(x))\n",
    "        h = torch.relu(self.fc_hidden(h))\n",
    "        mu = self.fc_mu(h)\n",
    "        log_sigma = self.fc_sigma(h)\n",
    "        z = self.reparameterization(mu, log_sigma)\n",
    "\n",
    "        return z, mu, log_sigma\n",
    "    \n",
    "    def reparameterization(self, mu, log_sigma):\n",
    "        sigma = torch.exp(log_sigma)\n",
    "        epsilon = torch.rand_like(sigma).to(DEVICE)\n",
    "        z = mu + sigma * epsilon\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc_hidden1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc_hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_output = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = torch.relu(self.fc_hidden1(x))\n",
    "        h = torch.relu(self.fc_hidden2(h))\n",
    "        x_reconstr = torch.sigmoid(self.fc_output(h))\n",
    "        return x_reconstr\n",
    "    \n",
    "    \n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "                \n",
    "    def forward(self, x):\n",
    "        z, mu, log_sigma = self.encoder(x)\n",
    "        x_reconstr = self.decoder(z)\n",
    "        \n",
    "        return x_reconstr, mu, log_sigma\n",
    "    \n",
    "\n",
    "x_dim  = 300\n",
    "hidden_dim = 500\n",
    "latent_dim = 2  # NOTE: a 2-dimensional latent space allows for nice latent space plots, but is too low-dimensional to perform well in general\n",
    "\n",
    "cuda = True  # NOTE: if running in Google Colab, make sure to go to \"Edit > Notebook settings\" and set \"Hardware accelerator\" to \"GPU\"\n",
    "DEVICE = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\n",
    "decoder = Decoder(latent_dim=latent_dim, hidden_dim=hidden_dim, output_dim=x_dim)\n",
    "\n",
    "vae = VAE(encoder=encoder, decoder=decoder).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_ee8w-ZC1jJD"
   },
   "outputs": [],
   "source": [
    "## Download the anomaly evaluation data ##\n",
    "data_2 = download('https://surfdrive.surf.nl/files/index.php/s/EzMkh3SZbsbJb2i/download', filetype='tensor')\n",
    "is_anomaly = download('https://surfdrive.surf.nl/files/index.php/s/wrK5xipcIC9DHhu/download', filetype='pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1621940470580,
     "user": {
      "displayName": "Yoeri Poels",
      "photoUrl": "",
      "userId": "12918185417432069249"
     },
     "user_tz": -120
    },
    "id": "IXVaLNjF-mFi",
    "outputId": "0bdf7c85-58f3-4936-ec67-688a160b698a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num new nodes: 2000\n",
      "Num new edges: 50176\n"
     ]
    }
   ],
   "source": [
    "total_node = data_2.x.shape[0]\n",
    "total_edge = data_2.edge_index.shape[1]\n",
    "new_node = total_node - num_node\n",
    "new_edge = total_edge - num_edge\n",
    "print(f'Num new nodes: {new_node}')\n",
    "print(f'Num new edges: {new_edge}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1621940470581,
     "user": {
      "displayName": "Yoeri Poels",
      "photoUrl": "",
      "userId": "12918185417432069249"
     },
     "user_tz": -120
    },
    "id": "8mVXRMxPA_oQ",
    "outputId": "ba3fabfb-e0fb-4351-d6fd-8c26b8e477af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of new nodes are 10701...12700\n",
      "The first 1000 are real articles, the last 1000 are anomaly articles\n",
      "You can use the map \"is_anomaly\" to identify whether a node is an anomaly or not\n",
      "E.g., is_anomaly[11201] = 0 and is_anomaly[12201] = 1\n"
     ]
    }
   ],
   "source": [
    "print(f'Indices of new nodes are {num_node}...{total_node-1}')\n",
    "print('The first 1000 are real articles, the last 1000 are anomaly articles')\n",
    "print('You can use the map \"is_anomaly\" to identify whether a node is an anomaly or not')\n",
    "e_real = num_node+500\n",
    "e_anomaly = num_node+1500\n",
    "print(f'E.g., is_anomaly[{e_real}] = {is_anomaly[e_real]} and is_anomaly[{e_anomaly}] = {is_anomaly[e_anomaly]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(data_1.x[:10700], data_1.y[:10700])\n",
    "test_dataset = TensorDataset(data_2.x, data_2.y)\n",
    "\n",
    "batch_size = 100\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} \n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "def loss_function(x, x_reconstr, mu, log_sigma):\n",
    "    reconstr_loss = nn.functional.mse_loss(x_reconstr, x, reduction='sum')\n",
    "    kl_loss = 0.5 * torch.sum(mu.pow(2) + (2*log_sigma).exp() - 2*log_sigma - 1)\n",
    "    total_loss = reconstr_loss + kl_loss\n",
    "    return total_loss, reconstr_loss, kl_loss\n",
    "\n",
    "optimizer = Adam(vae.parameters(), lr=lr)\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "print(\"Start training VAE...\")\n",
    "vae.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    overall_loss = 0\n",
    "    overall_reconstr_loss = 0\n",
    "    overall_kl_loss = 0\n",
    "    for batch_idx, (x, _) in enumerate(train_loader):\n",
    "        \n",
    "        x = x.view(batch_size, x_dim)\n",
    "        x = x.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_reconstr, mu, log_sigma = vae(x)\n",
    "        loss, reconstr_loss, kl_loss = loss_function(x, x_reconstr, mu, log_sigma)\n",
    "        \n",
    "        overall_loss += loss.item()\n",
    "        overall_reconstr_loss += reconstr_loss.item()\n",
    "        overall_kl_loss += kl_loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    n_datapoints = batch_idx * batch_size\n",
    "    print(\"\\tEpoch\", epoch + 1, \"\\tAverage Loss: \", overall_loss / n_datapoints, \"\\tReconstruction Loss:\", overall_reconstr_loss / n_datapoints, \"\\tKL Loss:\", overall_kl_loss / n_datapoints)\n",
    "    \n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wzWb42RX1myv"
   },
   "outputs": [],
   "source": [
    "## Evaluate the anomaly detection model ##\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMtIUUNA5rp626+rMGY54T9",
   "collapsed_sections": [],
   "name": "A2_Skeleton.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
